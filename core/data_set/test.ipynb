{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分离完成，已生成'bad_urls.txt'和'good_urls.txt'\n"
     ]
    }
   ],
   "source": [
    "#处理csv数据集为txt文件以供规则匹配\n",
    "import csv\n",
    "\n",
    "# CSV文件路径，请确保替换为您具体的文件路径\n",
    "csv_file_path = '.\\csv\\phishing_site_urls.csv'\n",
    "\n",
    "# 输出TXT文件的路径\n",
    "bad_txt_path = 'bad_urls.txt'\n",
    "good_txt_path = 'good_urls.txt'\n",
    "\n",
    "# 初始化文件写入对象\n",
    "with open(bad_txt_path, 'w',encoding='utf-8') as bad_file, open(good_txt_path, 'w',encoding='utf-8') as good_file:\n",
    "    # 读取CSV文件\n",
    "    with open(csv_file_path, mode='r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        \n",
    "        for row in csv_reader:\n",
    "            url = row['URL']\n",
    "            label = row['Label']\n",
    "            \n",
    "            # 根据标签写入对应的文件\n",
    "            if label == 'bad':\n",
    "                bad_file.write(url + '\\n')\n",
    "            elif label == 'good':\n",
    "                good_file.write(url + '\\n')\n",
    "\n",
    "print(\"分离完成，已生成'bad_urls.txt'和'good_urls.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35537190082644626\n"
     ]
    }
   ],
   "source": [
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]\n",
    "\n",
    "# 计算相似度百分比\n",
    "def similarity_by_levenshtein(s1, s2):\n",
    "    distance = levenshtein_distance(s1, s2)\n",
    "    max_len = max(len(s1), len(s2))\n",
    "    similarity = (max_len - distance) / max_len\n",
    "    return similarity\n",
    "\n",
    "str1 = \"pub-ea806534a1b74cd39c327bf44e5f76f4.r2.dev/Dw7jxsWCu3bn9wkDwlm3ntV0jGD4vtzwk93mXiejjS8o0wjG75n93keChru7mI3XwuO92jn5I.htm\"\n",
    "str2 = \"pub-ea806534a1b74cd39c327bf44e5f76f4.r2.dev\"\n",
    "print(similarity_by_levenshtein(str1, str2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18257418583505539\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_similarity_str(s1, s2):\n",
    "    vectorizer = CountVectorizer().fit_transform([s1, s2])\n",
    "    vectors = vectorizer.toarray()\n",
    "    return cosine_similarity(vectors)[0][1]\n",
    "\n",
    "str1 = \"kimki.ru/emeka/panelnew/gate.php\"\n",
    "str2 = \"www.alfalima.it/transactions.php\"\n",
    "print(cosine_similarity_str(str1, str2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emeka</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gate</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kimki</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>panelnew</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>php</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ru</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word     tfidf\n",
       "0     emeka  0.408248\n",
       "1      gate  0.408248\n",
       "2     kimki  0.408248\n",
       "3  panelnew  0.408248\n",
       "4       php  0.408248\n",
       "5        ru  0.408248"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "#取出X和y\n",
    "X = ['kimki.ru/emeka/panelnew/gate.php']\n",
    "y = [1]\n",
    "#创建一个TfidfVectorizer的实例\n",
    "vectorizer = TfidfVectorizer()\n",
    "#使用Tfidf将文本转化为向量\n",
    "X = vectorizer.fit_transform(X)\n",
    "#看看特征形状\n",
    "X.shape\n",
    "\n",
    "data1 = {'word': vectorizer.get_feature_names_out(),\n",
    "        'tfidf': X.toarray().sum(axis=0).tolist()}\n",
    "df1 = pd.DataFrame(data1).sort_values(by=\"tfidf\" ,ascending=False,ignore_index=True) \n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练用于识别的机器学习模型\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['font.sans-serif'] = ['KaiTi']  #指定默认字体 SimHei黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False   #解决保存图像是负号'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# 读取数据\n",
    "data = pd.read_csv('.\\csv\\phishing_site_urls.csv')  \n",
    "X = data['URL'].values\n",
    "y = data['Label'].map({'bad': 0, 'good': 1}).values  # 将标签转换为0和1\n",
    "#创建一个TfidfVectorizer的实例\n",
    "vectorizer = TfidfVectorizer()\n",
    "#使用Tfidf将文本转化为向量\n",
    "X = vectorizer.fit_transform(X)\n",
    "#看看特征形状\n",
    "X.shape\n",
    "\n",
    "# 保存vectorizer到文件\n",
    "with open('./model_save/tfidf_vectorizer.pickle', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {'word': vectorizer.get_feature_names_out(),\n",
    "        'tfidf': X.toarray().sum(axis=0).tolist()}\n",
    "df1 = pd.DataFrame(data1).sort_values(by=\"tfidf\" ,ascending=False,ignore_index=True) \n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((439476, 528563), (109870, 528563), (439476,), (109870,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X,y,test_size=0.2,stratify=y,random_state = 0)\n",
    "#可以检查一下划分后数据形状\n",
    "X_train.shape,X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#采用十种模型，对比测试集精度\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "逻辑回归方法在测试集的准确率为0.98\n",
      "逻辑回归模型已保存至逻辑回归.pkl\n",
      "朴素贝叶斯方法在测试集的准确率为0.973\n",
      "朴素贝叶斯模型已保存至朴素贝叶斯.pkl\n",
      "K近邻模型训练出错：Unable to allocate 266. MiB for an array with shape (69771299,) and data type int32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#逻辑回归\n",
    "model1 =  LogisticRegression(C=1e10,max_iter=10000)\n",
    " \n",
    "#朴素贝叶斯\n",
    "model2 = MultinomialNB()\n",
    " \n",
    "#K近邻\n",
    "model3 = KNeighborsClassifier(n_neighbors=50)\n",
    " \n",
    "#决策树\n",
    "model4 = DecisionTreeClassifier(random_state=77)\n",
    " \n",
    "#随机森林\n",
    "model5= RandomForestClassifier(n_estimators=500,  max_features='sqrt',random_state=10)\n",
    " \n",
    "#梯度提升\n",
    "model6 = GradientBoostingClassifier(random_state=123)\n",
    " \n",
    " \n",
    "#支持向量机\n",
    "model9 = SVC(kernel=\"rbf\", random_state=77)\n",
    " \n",
    "#神经网络\n",
    "model10 = MLPClassifier(hidden_layer_sizes=(16,8), random_state=77, max_iter=10000)\n",
    " \n",
    "model_list=[model1,model2,model3,model4,model5,model6,model9,model10]\n",
    "model_name=['逻辑回归','朴素贝叶斯','K近邻','决策树','随机森林','梯度提升','支持向量机','神经网络']\n",
    "scores=[]\n",
    "for i in range(len(model_list)):\n",
    "    try:\n",
    "        model_C=model_list[i]\n",
    "        name=model_name[i]\n",
    "        model_C.fit(X_train, y_train)\n",
    "        s=model_C.score(X_test, y_test)\n",
    "        scores.append(s)\n",
    "        print(f'{name}方法在测试集的准确率为{round(s,3)}')\n",
    "        # 保存模型\n",
    "        model_path = f\"./model_save/{name}.pkl\"  # 修改文件扩展名为.pkl\n",
    "        with open(model_path, 'wb') as file:\n",
    "            pickle.dump(model_C, file)\n",
    "        print(f\"{name}模型已保存至{model_path}\")   \n",
    "    except Exception as e:\n",
    "        print(f\"{name}模型训练出错：{e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,3),dpi=128)\n",
    "sns.barplot(y=model_name,x=scores,orient=\"h\")\n",
    "plt.xlabel('模型准确率')\n",
    "plt.ylabel('模型名称')\n",
    "plt.xticks(fontsize=10,rotation=45)\n",
    "plt.title(\"不同模型文本分类准确率对比\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果1:[0]\n",
      "预测结果2:[1]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def predict_with_pickle(model_path, input_vector):\n",
    "    \"\"\"使用pickle加载模型并进行预测\"\"\"\n",
    "    with open(model_path, 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "        prediction = loaded_model.predict(input_vector)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "\n",
    "#预处理文本\n",
    "def preprocess_text_to_vector(text):\n",
    "    # 从文件加载vectorizer\n",
    "    with open('./model_save/tfidf_vectorizer.pickle', 'rb') as file:\n",
    "        vectorizer = pickle.load(file)\n",
    "        #使用Tfidf将文本转化为向量\n",
    "        X = vectorizer.transform([text])\n",
    "        return X\n",
    "\n",
    "\n",
    "# 示例：使用第一个模型（逻辑回归）进行预测\n",
    "str1 = \"haldforsamlingshus.dk/spiritual/DHL/index.php?l=_JeHFUq_VJOXK0QWHtoGYDw_Product-UserID&amp;;;userid\"\n",
    "str2 = \"associations2.html\"\n",
    "predicted_labels = predict_with_pickle('./model_save/逻辑回归.pkl', preprocess_text_to_vector(str1))\n",
    "predicted_labels2 = predict_with_pickle('./model_save/逻辑回归.pkl', preprocess_text_to_vector(str2))\n",
    "print(f\"预测结果1:{predicted_labels}\")\n",
    "print(f\"预测结果2:{predicted_labels2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
